{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# PRACTICA NLP. NOTEBOOK 4\n",
    "\n",
    "## 4. Reporte de métricas y conclusiones\n",
    "\n",
    "El alumno, tomando como referencia los resultados del modelo escogido en el ejercicio 3, calculará las métricas que permitan validar la bondad del modelo. \n",
    "\n",
    "También incluirá comentarios y las conclusiones finales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Del notebook anterior concluyo que, aunque mejorable, el mejor modelo que he probado es una clasificación random forest, con el preprocesado definido en el notebook 2 y los valores de algunos parámetros del tfidf ajustados.\n",
    "\n",
    "La matriz de confusión en la que se muestran los resultados de aciertos y fracasos en la predicción es la siguiente: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>804</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>276</td>\n",
       "      <td>736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall  0.0  1.0\n",
       "0      0.0  804  184\n",
       "1      1.0  276  736"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "conf_matrix= pd.read_csv('confusion_matrix.csv', sep=',', decimal='.')\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>0.0</td>\n",
       "      <td>804</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>1.0</td>\n",
       "      <td>276</td>\n",
       "      <td>736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          label  negative  positive\n",
       "negative    0.0       804       184\n",
       "positive    1.0       276       736"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#retoco los nombres de las columnas y filas de la matriz para facilitar la comprensión\n",
    "conf_matrix.rename(columns={'overall': 'label', '0.0': 'negative','1.0':'positive'}, inplace=True)\n",
    "conf_matrix.set_axis(['negative','positive'], axis='index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label &amp; global_metrics</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.744444</td>\n",
       "      <td>0.813765</td>\n",
       "      <td>0.777563</td>\n",
       "      <td>988.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>1012.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>promedio</th>\n",
       "      <td>macro avg</td>\n",
       "      <td>0.772222</td>\n",
       "      <td>0.770519</td>\n",
       "      <td>0.769734</td>\n",
       "      <td>2000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>media ponderada</th>\n",
       "      <td>weighted avg</td>\n",
       "      <td>0.772556</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>0.769640</td>\n",
       "      <td>2000.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                label & global_metrics  precision    recall  f1-score  support\n",
       "negative                           0.0   0.744444  0.813765  0.777563   988.00\n",
       "positive                           1.0   0.800000  0.727273  0.761905  1012.00\n",
       "accuracy                      accuracy   0.770000  0.770000  0.770000     0.77\n",
       "promedio                     macro avg   0.772222  0.770519  0.769734  2000.00\n",
       "media ponderada           weighted avg   0.772556  0.770000  0.769640  2000.00"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hago lo mismo con el classification report\n",
    "classification_report= pd.read_csv('classification_report.csv', sep=',', decimal='.')\n",
    "classification_report.rename(columns={'Unnamed: 0': 'label & global_metrics'}, inplace=True)\n",
    "classification_report.set_axis(['negative','positive','accuracy','promedio','media ponderada'], axis='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se seleccionaron para entrenar el modelo 10000 entradas del dataset de reviews de instrumentos musicales, 5000 con valoración positiva y 5000 con valoración negativa, para poder entrenar un modelo balanceado.\n",
    "\n",
    "De estas valoraciones 2000 se reservaron para la fase de test y 8000 se utilizaron para el entrenamiento, como vimos en el notebook anterior. \n",
    "\n",
    "En la matriz de confusión y en el classification report podemos observar que el conjunto de test tiene 988 valoraciones negativas y 1012 positivas. \n",
    "\n",
    "Después del entrenamiento los datos globales son los que vemos en la matriz de confusión: \n",
    "\n",
    "-el modelo interpreta correctamente como negativas 804 de las 988 y se equivoca en 184 que ve positivas cuando no lo son y\n",
    "\n",
    "-el modelo interpreta correctamente como positivas 736 y se equivoca en 276 que ve como negativas cuando no lo son. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El classification report muestra las métricas de precision, recall y f1-score por clase (en las dos primeras filas, 0-negativa, 1-positiva, puesto que se trata de una clasificación binaria).\n",
    "Las métricas se calculan utilizando los valores de la matriz de confusión, true y false positives, y true y false negatives. \n",
    "Hay 4 maneras de comprobar si las predicciones son o no correctas:\n",
    "\n",
    "TN / True Negative: review negativa predicha como negativa\n",
    "TP / True Positive: review positiva y predicha positiva\n",
    "FN / False Negative: review positiva pero predicha negativa\n",
    "FP / False Positive: review negativa pero predicha positiva\n",
    "\n",
    "\n",
    "Precision: ¿qué porcentaje de las predicciones son correctas? \n",
    "Recall: qué porcentaje de las review positivas detecta el modelo? \n",
    "F1 score: qué porcentaje de predicciones positivas eran correctas? \n",
    "\n",
    "Precision = TP/(TP + FP)\n",
    "Recall = TP/(TP+FN)\n",
    "F1 Score = 2*(Recall * Precision) / (Recall + Precision)\n",
    "\n",
    "##### Resumen de resultados: \n",
    "\n",
    "El modelo identifica correctamente el sentimiento de una review el 77% del tiempo. ->accuracy\n",
    "Cuando ha predicho una review como positiva, esa review lo era realmente el 80% de las veces -> precision\n",
    "Cuando una review era positiva, el modelo lo ha identificado correctamente en el 72.7% de los casos.->recall\n",
    "\n",
    "macro-avg es el promedio de precision/recall/F1 de todas las clases y weighted avg la media ponderada del total de TP/total de reviews: ambas 0.77 en este caso.\n",
    "\n",
    "Aunque era la mejor versión conseguida hasta el momento en cuanto a resultados comparado con otros modelos, se puede mejorar mucho. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Conclusiones Globales y Mejoras posibles:\n",
    "  \n",
    " - El resumen de resultados del punto anterior es una buena conclusión. Teniendo en cuenta que detecto muchas cosas aún por hacer, más que concluir me centro en las mejoras. \n",
    " \n",
    " - En primer lugar, empezando por el final, en los modelos se podía haber jugado con los valores de los parámetros, además de probar diferentes modelos y modificar los valores del tfidf en bloque (versión default/customizada)\n",
    " \n",
    " - También tenía localizado un modelo preentrenado de redes neuronales que quería probar.. para otra ocasión.\n",
    " \n",
    " - Por otro lado, se ha filtrado el número de reviews que se han utilizado para poder obtener resultados de una manera más rápida en los cálculos, pero seguro que con más tiempo se puede hacer con una mayor cantidad de datos en la muestra a pesar de las limitaciones del pc. Otra cosa que se podría haber probado ahora al final con el modelo, es introducirle nuevos datos de los que han quedado fuera y ver qué tal generaliza.\n",
    " \n",
    " - En cuanto al dataset, decidí en un primer momento juntar el summary con el texto de la review y quizá no ha ayudado al resultado al realizar el filtro. De hecho, una idea de mejora es equilibrar el número de caracteres de las reviews. Me queda pendiente hacer pruebas limitando el número de caracteres, creo que hay reviews muy largas difíciles de interpretar. \n",
    " \n",
    " - Como ya comentaba en el notebook del preprocesado, encontré un listado de stopwords  muy completo pero de un estudio realizado sobre el nytimes. de haber tenido más tiempo habría localizado un listado de stopwords más informal. Finalmente para una de las versiones trabajé con el listado descargado y para otra con el listado de spacy. Esta última es la que ha quedado para el notebook 3. Me hubiera gustado probar con el mío además de buscar otro.\n",
    " \n",
    " - En el primer notebook planteé la opción de revisar si influyen las personas en el tipo de review (si los que escriben muchas reviews tienden a poner la mayoría positivas o negativas...a alguna marca en particular..tipos de personas, de influencias..)...y si hay algún efecto observable en las reviews. Para prácticas futuras. \n",
    " \n",
    " - Por último, como también comentaba en un principio, encontré dos listados de palabras uno formado por palabras positivas y otro formado por palabras negativas que me hubiera gustado utilizar para analizar las valoraciones iniciales con un 3, para decidir si las asociaba a la valoración negativa o positiva con un fundamento más allá de mi propio criterio. \n",
    "  \n",
    " ... Tampoco he podido probar con los otros dos datasets descargados, para valorar la capacidad de generalización del modelo. \n",
    " \n",
    " Muchas cosas por mejorar, pero muy contenta de haber llegado hasta aquí. \n",
    " Gracias!\n",
    " \n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
